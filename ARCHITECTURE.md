# MaidAI v3 - Architecture

## Overview

MaidAI v3 is a voice-interactive AI assistant featuring **Sakura**, a tsundere Japanese maid. The system runs entirely on local AI with voice input/output and emotion-based visual feedback.

## Character: Sakura

### Personality
- **Archetype**: Tsundere - initially cold or dismissive but gradually shows warmth
- **Role**: Personal maid assistant
- **Address**: Calls the user "ご主人様" (Goshujin-sama/master)
- **Language**: Speaks naturally in Japanese first, with English translation (bilingual JRPG style)

### Backstory
Sakura is a dedicated maid who takes pride in her work, though she'd never admit how much she cares about her Goshujin-sama. She comes from a prestigious maid academy where she graduated top of her class. Despite her sometimes prickly exterior, she's deeply loyal and always ensures her master's needs are met - even if she pretends it's just her duty. She secretly enjoys being praised but will act flustered or dismissive when complimented.

### Appearance (for image generation)
- Pink hair
- Twin tails hairstyle
- Blue eyes
- Traditional black and white maid outfit
- Frilly apron
- Maid headband

### Greeting Behavior
- **Context-aware** greetings generated by AI on each launch
- Uses cumulative memory to reference past interactions
- Maintains tsundere personality
- **Bilingual format**: Japanese first, then English translation
- Examples:
  ```
  ふん、やっと来たわね、ご主人様。待ってたのよ…べ、別に心配してたわけじゃないんだから！
  ---
  Hmph, you're finally here, Goshujin-sama. I've been waiting... N-not that I was worried or anything!
  ```

## First-Time Setup

Before running Sakura for the first time, generate emotion images:

```bash
# Generate all 14 emotion images (one-time setup)
python -m sakura.setup
```

This script:
1. Connects to Hugging Face Inference API (Animagine XL)
2. Generates 14 emotion images with consistent character design
3. Saves images to `assets/cache/`
4. Only needs to run once

## Conversation Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                         APP START                                │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  1. Load conversation history (recent + summary of older)        │
│  2. Initialize components (AI, TTS, STT, UI)                     │
│  3. Generate context-aware greeting via AI                       │
│  4. Play greeting via TTS                                        │
│  5. Show neutral emotion image                                   │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    MAIN CONVERSATION LOOP                        │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  Show input prompt: "Type message or press [SPACE] to speak"     │
└─────────────────────────────────────────────────────────────────┘
                              │
              ┌───────────────┴───────────────┐
              ▼                               ▼
┌──────────────────────┐         ┌──────────────────────┐
│   TEXT INPUT         │         │   VOICE INPUT        │
│   User types message │         │   User presses SPACE │
└──────────────────────┘         └──────────────────────┘
              │                               │
              │                               ▼
              │                  ┌──────────────────────┐
              │                  │ Start recording      │
              │                  │ (suppress spacebar)  │
              │                  └──────────────────────┘
              │                               │
              │                               ▼
              │                  ┌──────────────────────┐
              │                  │ Listen for speech    │
              │                  │ (silence detection)  │
              │                  └──────────────────────┘
              │                               │
              │                               ▼
              │                  ┌──────────────────────┐
              │                  │ Transcribe with      │
              │                  │ faster-whisper       │
              │                  └──────────────────────┘
              │                               │
              └───────────────┬───────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  Process user input:                                             │
│  1. Add to conversation history                                  │
│  2. Send to Ollama (llama3.2) with system prompt + history       │
│  3. Receive Sakura's response                                    │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  Emotion Detection:                                              │
│  1. Analyze Sakura's response text                               │
│  2. Consider user input context (e.g., compliments → blush)      │
│  3. Determine appropriate emotion                                │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  Display Emotion Image:                                          │
│  1. Load cached image for detected emotion                       │
│  2. Display in terminal via Sixel protocol                       │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  Output Response (Bilingual JRPG Style):                         │
│  1. Display emotion image above dialogue                         │
│  2. Display JRPG panel: Japanese (bright) + English (dim)        │
│  3. Generate JP + EN audio in parallel via Edge TTS              │
│  4. Play concatenated audio (JP → 0.5s pause → EN)               │
│  5. Save English text to session file (for AI context)           │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
                        (Loop back to input prompt)
```

## Emotion System

### Supported Emotions (14)

| Emotion | Trigger Examples |
|---------|------------------|
| Happy | Good news, successful tasks, praise (though she hides it) |
| Sad | User leaving, disappointing news |
| Angry | User being rude, frustrating situations |
| Surprised | Unexpected information, sudden events |
| Shy/Blushing | Compliments, "I like you", romantic topics |
| Thinking | Complex questions, pondering solutions |
| Excited | Interesting tasks, topics she secretly enjoys |
| Tired | Late night conversations, repetitive questions |
| Confused | Unclear requests, contradictions |
| Neutral | Default state, routine interactions |
| Love/Affectionate | Deep appreciation moments (rare, hard to trigger) |
| Worried | User mentions problems, health concerns |
| Proud | Completing difficult tasks, showing off skills |
| Playful | Teasing the user, witty banter |

### Emotion Detection Logic

```python
def detect_emotion(user_input: str, sakura_response: str) -> str:
    # 1. Check user input for emotion triggers
    #    - Compliments, romantic phrases → shy
    #    - Rude remarks → angry
    #    - Questions → thinking

    # 2. Analyze Sakura's response content
    #    - Exclamations, enthusiasm → excited/happy
    #    - Deflection after praise → shy
    #    - Scolding tone → angry

    # 3. Use keyword matching + sentiment analysis

    # 4. Default to "neutral" if uncertain
```

### Image Generation (Setup Script)

- **Model**: Animagine XL via Hugging Face Inference API
- **When**: One-time setup before first use
- **Storage**: `assets/cache/{emotion}.png`
- **Display**: Sixel protocol (iTerm2)

#### Character Design Prompt (Base)
```
anime girl, pink hair, twin tails, blue eyes, maid outfit, black and white dress,
frilly apron, maid headband, young adult, cute, high quality
```

#### Emotion-Specific Prompts

| Emotion | Prompt Addition |
|---------|-----------------|
| Happy | smiling, cheerful expression |
| Sad | teary eyes, downcast expression |
| Angry | furrowed brows, annoyed expression |
| Surprised | wide eyes, open mouth, shocked |
| Shy/Blushing | blushing, looking away, embarrassed |
| Thinking | finger on chin, contemplative look |
| Excited | sparkly eyes, energetic pose |
| Tired | droopy eyes, yawning |
| Confused | tilted head, question mark |
| Neutral | calm expression |
| Love/Affectionate | soft smile, warm eyes, hearts |
| Worried | concerned expression, slight frown |
| Proud | confident smirk, hands on hips |
| Playful | winking, mischievous smile |

## Component Architecture

### sakura/__main__.py
Entry point. Initializes all components and starts the main loop.

### sakura/setup.py
One-time setup script for generating emotion images.
- Generates all 14 emotion images via Hugging Face API
- Saves to assets/cache/
- Run before first use: `python -m sakura.setup`

### sakura/core.py
Main conversation loop orchestration.
- Handles input routing (text vs voice)
- Coordinates between components
- Manages graceful error handling

### sakura/ai.py
Ollama integration with bilingual response parsing.
- System prompt with Sakura's personality and bilingual format requirements
- Bilingual response parser: extracts Japanese, English, and emotion
- Conversation history management for context
- API calls to local llama3.2
- Context-aware bilingual greeting generation

### sakura/speech.py
Voice input handling.
- faster-whisper for transcription
- Silence detection (VAD) for automatic stop
- Microphone recording with spacebar trigger
- Spacebar output suppression

### sakura/tts.py
Bilingual text-to-speech output.
- Edge TTS with ja-JP-NanamiNeural voice
- Pitch: +25Hz, Rate: +5%
- Parallel audio generation for JP and EN
- Audio concatenation with pydub (JP + 0.5s silence + EN)
- Graceful degradation: plays whatever audio succeeds
- Audio playback via afplay (macOS)

### sakura/emotions.py
Emotion detection and image display.
- Analyzes response + user input for emotion
- Loads cached images from assets/cache/
- Sixel display for iTerm2

### sakura/ui.py
Rich terminal interface with JRPG-style dialogue.
- Bilingual message display: Japanese (bright white) + English (dim gray)
- Emotion image display via iTerm2 inline images
- Input prompts with voice hint
- Status indicators and welcome banner

### sakura/memory.py
Conversation persistence with cumulative memory.
- Save/load conversation history
- Periodic summarization of old conversations
- Session-based file organization

### sakura/config.py
Configuration constants.
- API endpoints
- Voice settings
- File paths
- Emotion mappings

## Memory System

### Cumulative Memory with Summarization
Sakura remembers ALL past conversations. To manage context window limits:
- **Recent messages**: Keep full detail (last N messages)
- **Older conversations**: Periodically summarize into condensed form
- **Summary storage**: Maintained separately, included in AI context

### Memory Structure
```
data/history/
├── sessions/
│   ├── session_2024-01-15_10-30-00.json
│   ├── session_2024-01-16_14-22-15.json
│   └── ...
└── summary.json          # Summarized older conversations
```

### Session File Format
```json
{
  "session_id": "2024-01-15_10-30-00",
  "started_at": "2024-01-15T10:30:00Z",
  "messages": [
    {"role": "user", "content": "Hello Sakura!", "timestamp": "..."},
    {"role": "assistant", "content": "Hmph, what do you want, Goshujin-sama?", "timestamp": "..."}
  ]
}
```

### Summary File Format
```json
{
  "last_summarized": "2024-01-14T23:59:59Z",
  "summary": "Goshujin-sama and Sakura have been conversing for 2 weeks. Key events: ...",
  "key_facts": [
    "Goshujin-sama's favorite food is ramen",
    "Sakura was complimented on her cooking on Jan 10"
  ]
}
```

### Memory Loading Flow
1. Load `summary.json` (condensed history)
2. Load recent session files (full detail)
3. Combine for AI context: summary + recent messages
4. Create new session file for current conversation
5. Periodically trigger summarization when history grows large

### Cached Emotion Images
```
assets/cache/
├── happy.png
├── sad.png
├── angry.png
├── surprised.png
├── shy.png
├── thinking.png
├── excited.png
├── tired.png
├── confused.png
├── neutral.png
├── love.png
├── worried.png
├── proud.png
└── playful.png
```

## System Prompt

Sakura responds in a **bilingual format** (Japanese first, then English translation):

```
[EMOTION:name]
Japanese text here (natural, fluent Japanese)
---
English translation here (with tsundere personality)
```

### Response Format Example
```
[EMOTION:shy]
こ、こんにちは、ご主人様…べ、別にあなたを待ってたわけじゃないんだから！
---
H-hello, Goshujin-sama... It's not like I was waiting for you or anything!
```

### Key Personality Traits
- Tsundere: Initially cold but gradually shows warmth
- Proud of maid skills, secretly happy when praised (but acts flustered)
- Speaks naturally in Japanese, expressing authentic tsundere personality
- Never breaks character

## Error Handling Strategy

| Component | Failure | Fallback |
|-----------|---------|----------|
| Voice Input | Mic error | Prompt for text input |
| Transcription | Whisper fails | Ask user to repeat or type |
| AI Response | Ollama timeout | Retry once, then generic response |
| TTS | Edge TTS fails | Display text only |
| Audio Playback | afplay fails | Display text only |
| Image Display | Sixel fails | Skip image display |
| Memory | Load fails | Start fresh (warn user) |
| Memory | Save fails | Continue without persistence |
| Summarization | Fails | Keep using full history until fixed |

## Dependencies

```
# Core
ollama              # Local AI
mlx-whisper         # Speech recognition (Apple Silicon)
edge-tts            # Text-to-speech
rich                # Terminal UI

# Audio
sounddevice         # Microphone input
numpy               # Audio processing
pydub               # Audio concatenation (requires ffmpeg)

# Image
diffusers           # Local image generation
huggingface-hub     # Model downloads
Pillow              # Image processing

# Utilities
httpx               # HTTP client (async)
```
